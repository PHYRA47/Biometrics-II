{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpy in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpy, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `OpenPose`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=9jQGsUidKHs&t=1348s&ab_channel=DeepLearning_by_PhDScholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNetFromTensorflow('../models/graph_opt.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inWidth = 368\n",
    "inHeight = 368\n",
    "thr = 0.2\n",
    "\n",
    "BODY_PARTS = { \"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "                \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "                \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n",
    "                \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18 }\n",
    "\n",
    "POSE_PAIRS = [ [\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"], \n",
    "                [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"], \n",
    "                [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"], [\"Neck\", \"LHip\"], \n",
    "                [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"], [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"], \n",
    "                [\"REye\", \"REar\"], [\"Nose\", \"LEye\"], [\"LEye\", \"LEar\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_estimator(frame):\n",
    "    frameWidth = frame.shape[1]\n",
    "    frameHeight = frame.shape[0]\n",
    "    net.setInput(cv2.dnn.blobFromImage(\n",
    "        frame, 1.0, \n",
    "        (inWidth, inHeight), \n",
    "        (127.5, 127.5, 127.5), \n",
    "        swapRB=True, crop=False))\n",
    "    out = net.forward()\n",
    "    out = out[:, :19, :, :] # MobileNet output [1, 57, -1, -1], we only need the first 19 elements\n",
    "\n",
    "    assert(len(BODY_PARTS) == out.shape[1]) # net.getLayer(net.getLayerId('detection_out')).shapes[1]\n",
    "\n",
    "    points = []\n",
    "    for i in range(len(BODY_PARTS)):\n",
    "        # Slice heatmap of corresponding body's part\n",
    "        heatMap = out[0, i, :, :]\n",
    "\n",
    "        # Originally, we try to find all the local maximums. To simplify a sample\n",
    "        # we just find a global one. However only a single pose at the same time\n",
    "        # could be detected this way.   \n",
    "        _, conf, _, point = cv2.minMaxLoc(heatMap)\n",
    "        x = (frameWidth * point[0]) / out.shape[3]\n",
    "        y = (frameHeight * point[1]) / out.shape[2]\n",
    "\n",
    "        # Add a point if it's confidence is higher than threshold.\n",
    "        points.append((int(x), int(y)) if conf > thr else None)\n",
    "\n",
    "    for pair in POSE_PAIRS:\n",
    "        partFrom = pair[0]\n",
    "        partTo = pair[1]\n",
    "        assert(partFrom in BODY_PARTS)\n",
    "        assert(partTo in BODY_PARTS)\n",
    "\n",
    "        idFrom = BODY_PARTS[partFrom]\n",
    "        idTo = BODY_PARTS[partTo]\n",
    "\n",
    "        if points[idFrom] and points[idTo]:\n",
    "            cv2.line(frame, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
    "            cv2.ellipse(frame, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv2.FILLED)\n",
    "            cv2.ellipse(frame, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "    t, _ = net.getPerfProfile()\n",
    "    freq = cv2.getTickFrequency() / 1000\n",
    "    # cv2.putText(frame, '%.2fms' % (t / freq), (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0))\n",
    "    print('%.2fms' % (t / freq))\n",
    "\n",
    "    return points, frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125.19ms\n",
      "147.46ms\n",
      "151.16ms\n",
      "138.38ms\n",
      "133.98ms\n",
      "129.43ms\n",
      "129.54ms\n",
      "128.17ms\n",
      "122.19ms\n",
      "124.25ms\n",
      "128.24ms\n",
      "130.81ms\n",
      "130.75ms\n",
      "131.17ms\n",
      "130.90ms\n",
      "131.41ms\n",
      "125.89ms\n",
      "123.16ms\n",
      "128.00ms\n",
      "124.97ms\n",
      "124.04ms\n",
      "119.49ms\n",
      "125.49ms\n",
      "121.91ms\n",
      "126.15ms\n",
      "121.90ms\n",
      "123.75ms\n",
      "118.27ms\n",
      "133.70ms\n",
      "116.87ms\n",
      "130.21ms\n",
      "130.35ms\n",
      "134.25ms\n",
      "126.93ms\n",
      "126.33ms\n",
      "123.10ms\n",
      "130.28ms\n",
      "125.44ms\n",
      "126.47ms\n",
      "130.11ms\n",
      "125.86ms\n",
      "121.45ms\n",
      "128.51ms\n",
      "121.29ms\n",
      "125.16ms\n",
      "131.72ms\n",
      "134.23ms\n",
      "130.76ms\n",
      "135.56ms\n",
      "120.39ms\n",
      "125.63ms\n",
      "120.44ms\n",
      "129.78ms\n",
      "120.17ms\n",
      "121.36ms\n",
      "121.59ms\n",
      "124.32ms\n",
      "123.32ms\n",
      "129.87ms\n",
      "132.18ms\n",
      "133.99ms\n",
      "131.59ms\n",
      "135.69ms\n",
      "144.47ms\n",
      "134.14ms\n",
      "139.94ms\n",
      "138.75ms\n",
      "140.35ms\n",
      "138.66ms\n",
      "126.31ms\n",
      "126.30ms\n",
      "126.88ms\n",
      "128.07ms\n",
      "125.13ms\n",
      "129.28ms\n",
      "121.04ms\n",
      "120.09ms\n",
      "121.11ms\n",
      "123.34ms\n",
      "118.36ms\n",
      "124.28ms\n",
      "126.49ms\n",
      "132.82ms\n",
      "129.45ms\n",
      "129.73ms\n",
      "125.38ms\n",
      "122.44ms\n",
      "121.17ms\n",
      "123.70ms\n",
      "122.56ms\n",
      "123.51ms\n",
      "118.88ms\n",
      "122.19ms\n",
      "120.61ms\n",
      "121.80ms\n",
      "121.10ms\n",
      "129.33ms\n",
      "144.73ms\n",
      "147.53ms\n",
      "142.59ms\n",
      "135.67ms\n",
      "134.72ms\n",
      "126.90ms\n",
      "122.62ms\n",
      "121.91ms\n",
      "120.20ms\n",
      "128.33ms\n",
      "125.31ms\n",
      "124.84ms\n",
      "123.24ms\n",
      "130.97ms\n",
      "122.18ms\n",
      "126.19ms\n",
      "119.55ms\n",
      "122.20ms\n",
      "117.94ms\n",
      "124.71ms\n",
      "138.20ms\n",
      "129.96ms\n",
      "130.61ms\n",
      "118.64ms\n",
      "121.73ms\n",
      "119.75ms\n",
      "125.89ms\n",
      "122.72ms\n",
      "124.31ms\n",
      "124.25ms\n",
      "125.52ms\n",
      "130.42ms\n",
      "124.95ms\n",
      "131.00ms\n",
      "135.78ms\n",
      "139.26ms\n",
      "132.55ms\n",
      "133.72ms\n",
      "140.04ms\n",
      "139.58ms\n",
      "134.07ms\n",
      "135.43ms\n",
      "135.49ms\n",
      "139.20ms\n",
      "132.70ms\n",
      "131.96ms\n",
      "127.85ms\n",
      "123.56ms\n",
      "123.15ms\n",
      "128.12ms\n",
      "127.64ms\n",
      "130.63ms\n",
      "125.47ms\n",
      "131.57ms\n",
      "134.41ms\n",
      "132.91ms\n",
      "129.27ms\n",
      "131.98ms\n",
      "128.04ms\n",
      "127.30ms\n",
      "129.35ms\n",
      "128.37ms\n",
      "127.30ms\n",
      "129.55ms\n",
      "115.03ms\n",
      "121.36ms\n",
      "121.11ms\n",
      "117.04ms\n",
      "112.71ms\n",
      "116.79ms\n",
      "127.24ms\n",
      "128.67ms\n",
      "124.72ms\n",
      "129.34ms\n",
      "127.21ms\n",
      "127.15ms\n",
      "117.80ms\n",
      "122.49ms\n",
      "122.28ms\n",
      "121.33ms\n",
      "118.58ms\n",
      "121.77ms\n",
      "118.56ms\n",
      "121.72ms\n",
      "120.63ms\n",
      "123.92ms\n",
      "122.29ms\n",
      "125.56ms\n",
      "129.20ms\n",
      "125.90ms\n",
      "128.42ms\n",
      "126.74ms\n",
      "130.74ms\n",
      "130.07ms\n",
      "121.01ms\n",
      "127.58ms\n",
      "124.40ms\n",
      "122.85ms\n",
      "123.14ms\n",
      "126.36ms\n",
      "118.98ms\n",
      "121.61ms\n",
      "119.32ms\n",
      "123.35ms\n",
      "138.94ms\n",
      "146.57ms\n",
      "143.90ms\n",
      "133.05ms\n",
      "130.24ms\n",
      "127.29ms\n",
      "127.27ms\n",
      "125.51ms\n",
      "120.20ms\n",
      "121.48ms\n",
      "123.47ms\n",
      "123.07ms\n",
      "120.58ms\n",
      "131.16ms\n",
      "122.80ms\n",
      "127.07ms\n",
      "122.17ms\n",
      "125.04ms\n",
      "119.18ms\n",
      "121.75ms\n",
      "131.75ms\n",
      "127.14ms\n",
      "129.64ms\n",
      "132.85ms\n",
      "144.08ms\n",
      "138.89ms\n",
      "130.05ms\n",
      "134.06ms\n",
      "135.99ms\n",
      "134.50ms\n",
      "121.82ms\n",
      "127.16ms\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0) # 0 for web camera and inter video file path for video file\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 800)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 800)\n",
    "# cap.set(cv2.CAP_PROP_FPS, 10) # frame per second\n",
    "\n",
    "if not cap.isOpened():\n",
    "    cap.release()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "while cv2.waitKey(1) < 0:\n",
    "    hasFrame, frame = cap.read()\n",
    "    if not hasFrame:\n",
    "        cv2.waitKey()\n",
    "        break\n",
    "\n",
    "    _, frame = pose_estimator(frame)\n",
    "\n",
    "    cv2.imshow('OpenPose using OpenCV', frame)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `MoveNet: multipose-lightning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=KC7nJtBHBqg&t=2407s&ab_channel=NicholasRenotte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\froms\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\froms\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\froms\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\froms\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the model from TF Hub.\n",
    "model = hub.load(\"https://www.kaggle.com/models/google/movenet/TensorFlow2/multipose-lightning/1\")\n",
    "movenet = model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y, x, 1]))  \n",
    "\n",
    "    for kp in shaped:\n",
    "        ky, kx, kp_conf = kp\n",
    "        if kp_conf > confidence_threshold:\n",
    "            cv2.circle(frame, (int(kx), int(ky)), 4, (0, 255, 0), -1)\n",
    "\n",
    "EDGES = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
    "\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y, x, 1]))  \n",
    "\n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = shaped[p1]\n",
    "        y2, x2, c2 = shaped[p2]\n",
    "\n",
    "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 3)\n",
    "\n",
    "def loop_through_people(frame, keypoints_with_scores, edges, confidence_threshold):\n",
    "    for person in keypoints_with_scores:\n",
    "        draw_connections(frame, person, edges, confidence_threshold)\n",
    "        draw_keypoints(frame, person, confidence_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) # 0 for web camera and inter video file path for video file\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Resize image to a multiple of 32\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), # resize image to odd size with padding\n",
    "                                   192, 256)\n",
    "    input_img = tf.cast(img, dtype=tf.int32)\n",
    "\n",
    "    # Make detections\n",
    "    results = movenet(input_img)\n",
    "    keypoints_with_scores = results['output_0'].numpy()[:,:,:51].reshape((6, 17, 3))\n",
    "    # print(results)\n",
    "\n",
    "    # Draw the keypoints\n",
    "    loop_through_people(frame, keypoints_with_scores, EDGES, confidence_threshold=0.3)\n",
    "\n",
    "    cv2.imshow('Movenet Multipose ightning', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27: # ESC key\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `MoveNet: singlepose-lightning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=SSW9LzOJSus&ab_channel=NicholasRenotte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path='../models/movenet-singlepose-lightning_3.tflite')\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Reshape image\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(np.expand_dims(img, axis=0), 192,192)\n",
    "    input_image = tf.cast(img, dtype=tf.float32)\n",
    "    \n",
    "    # Setup input and output \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Make predictions \n",
    "    interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "    interpreter.invoke()\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    # Rendering \n",
    "    draw_connections(frame, keypoints_with_scores, EDGES, 0.4)\n",
    "    draw_keypoints(frame, keypoints_with_scores, 0.4)\n",
    "    \n",
    "    cv2.imshow('MoveNet Singlepose Lightning', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
