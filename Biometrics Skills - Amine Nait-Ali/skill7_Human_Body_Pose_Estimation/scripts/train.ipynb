{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\froms\\AppData\\Roaming\\Python\\Python39\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpy in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpy, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `OpenPose`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=9jQGsUidKHs&t=1348s&ab_channel=DeepLearning_by_PhDScholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNetFromTensorflow('../models/graph_opt.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inWidth = 368\n",
    "inHeight = 368\n",
    "thr = 0.2\n",
    "\n",
    "BODY_PARTS = { \"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "                \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "                \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n",
    "                \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18 }\n",
    "\n",
    "POSE_PAIRS = [ [\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"], \n",
    "                [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"], \n",
    "                [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"], [\"Neck\", \"LHip\"], \n",
    "                [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"], [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"], \n",
    "                [\"REye\", \"REar\"], [\"Nose\", \"LEye\"], [\"LEye\", \"LEar\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_estimator(frame):\n",
    "    frameWidth = frame.shape[1]\n",
    "    frameHeight = frame.shape[0]\n",
    "    net.setInput(cv2.dnn.blobFromImage(\n",
    "        frame, 1.0, \n",
    "        (inWidth, inHeight), \n",
    "        (127.5, 127.5, 127.5), \n",
    "        swapRB=True, crop=False))\n",
    "    out = net.forward()\n",
    "    out = out[:, :19, :, :] # MobileNet output [1, 57, -1, -1], we only need the first 19 elements\n",
    "\n",
    "    assert(len(BODY_PARTS) == out.shape[1]) # net.getLayer(net.getLayerId('detection_out')).shapes[1]\n",
    "\n",
    "    points = []\n",
    "    for i in range(len(BODY_PARTS)):\n",
    "        # Slice heatmap of corresponding body's part\n",
    "        heatMap = out[0, i, :, :]\n",
    "\n",
    "        # Originally, we try to find all the local maximums. To simplify a sample\n",
    "        # we just find a global one. However only a single pose at the same time\n",
    "        # could be detected this way.   \n",
    "        _, conf, _, point = cv2.minMaxLoc(heatMap)\n",
    "        x = (frameWidth * point[0]) / out.shape[3]\n",
    "        y = (frameHeight * point[1]) / out.shape[2]\n",
    "\n",
    "        # Add a point if it's confidence is higher than threshold.\n",
    "        points.append((int(x), int(y)) if conf > thr else None)\n",
    "\n",
    "    for pair in POSE_PAIRS:\n",
    "        partFrom = pair[0]\n",
    "        partTo = pair[1]\n",
    "        assert(partFrom in BODY_PARTS)\n",
    "        assert(partTo in BODY_PARTS)\n",
    "\n",
    "        idFrom = BODY_PARTS[partFrom]\n",
    "        idTo = BODY_PARTS[partTo]\n",
    "\n",
    "        if points[idFrom] and points[idTo]:\n",
    "            cv2.line(frame, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
    "            cv2.ellipse(frame, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv2.FILLED)\n",
    "            cv2.ellipse(frame, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "    t, _ = net.getPerfProfile()\n",
    "    freq = cv2.getTickFrequency() / 1000\n",
    "    # cv2.putText(frame, '%.2fms' % (t / freq), (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0))\n",
    "    print('%.2fms' % (t / freq))\n",
    "\n",
    "    return points, frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) # 0 for web camera and inter video file path for video file\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 800)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 800)\n",
    "# cap.set(cv2.CAP_PROP_FPS, 10) # frame per second\n",
    "\n",
    "if not cap.isOpened():\n",
    "    cap.release()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "while cv2.waitKey(1) < 0:\n",
    "    hasFrame, frame = cap.read()\n",
    "    if not hasFrame:\n",
    "        cv2.waitKey()\n",
    "        break\n",
    "\n",
    "    _, frame = pose_estimator(frame)\n",
    "\n",
    "    cv2.imshow('OpenPose using OpenCV', frame)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `MoveNet: multipose-lightning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=KC7nJtBHBqg&t=2407s&ab_channel=NicholasRenotte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\froms\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\froms\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\froms\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\froms\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the model from TF Hub.\n",
    "model = hub.load(\"https://www.kaggle.com/models/google/movenet/TensorFlow2/multipose-lightning/1\")\n",
    "movenet = model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y, x, 1]))  \n",
    "\n",
    "    for kp in shaped:\n",
    "        ky, kx, kp_conf = kp\n",
    "        if kp_conf > confidence_threshold:\n",
    "            cv2.circle(frame, (int(kx), int(ky)), 4, (0, 255, 0), -1)\n",
    "\n",
    "EDGES = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
    "\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y, x, 1]))  \n",
    "\n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = shaped[p1]\n",
    "        y2, x2, c2 = shaped[p2]\n",
    "\n",
    "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 3)\n",
    "\n",
    "def loop_through_people(frame, keypoints_with_scores, edges, confidence_threshold):\n",
    "    for person in keypoints_with_scores:\n",
    "        draw_connections(frame, person, edges, confidence_threshold)\n",
    "        draw_keypoints(frame, person, confidence_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) # 0 for web camera and inter video file path for video file\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Resize image to a multiple of 32\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), # resize image to odd size with padding\n",
    "                                   192, 256)\n",
    "    input_img = tf.cast(img, dtype=tf.int32)\n",
    "\n",
    "    # Make detections\n",
    "    results = movenet(input_img)\n",
    "    keypoints_with_scores = results['output_0'].numpy()[:,:,:51].reshape((6, 17, 3))\n",
    "    # print(results)\n",
    "\n",
    "    # Draw the keypoints\n",
    "    loop_through_people(frame, keypoints_with_scores, EDGES, confidence_threshold=0.3)\n",
    "\n",
    "    cv2.imshow('Movenet Multipose ightning', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27: # ESC key\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `MoveNet: singlepose-lightning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=SSW9LzOJSus&ab_channel=NicholasRenotte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path='../models/movenet-singlepose-lightning_3.tflite')\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Reshape image\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(np.expand_dims(img, axis=0), 192,192)\n",
    "    input_image = tf.cast(img, dtype=tf.float32)\n",
    "    \n",
    "    # Setup input and output \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Make predictions \n",
    "    interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "    interpreter.invoke()\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    # Rendering \n",
    "    draw_connections(frame, keypoints_with_scores, EDGES, 0.4)\n",
    "    draw_keypoints(frame, keypoints_with_scores, 0.4)\n",
    "    \n",
    "    cv2.imshow('MoveNet Singlepose Lightning', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == 27:\n",
    "        break\n",
    "        \n",
    "    t, _ = net.getPerfProfile()\n",
    "    freq = cv2.getTickFrequency() / 1000\n",
    "\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Reshape image\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(np.expand_dims(img, axis=0), 192,192)\n",
    "    input_image = tf.cast(img, dtype=tf.float32)\n",
    "    \n",
    "    # Setup input and output \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Make predictions \n",
    "    interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "    interpreter.invoke()\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    # Rendering \n",
    "    draw_connections(frame, keypoints_with_scores, EDGES, 0.4)\n",
    "    draw_keypoints(frame, keypoints_with_scores, 0.4)\n",
    "    \n",
    "    cv2.imshow('MoveNet Singlepose Lightning', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == 27:\n",
    "        break\n",
    "        \n",
    "    t, _ = net.getPerfProfile()\n",
    "    freq = cv2.getTickFrequency() / 1000\n",
    "\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
